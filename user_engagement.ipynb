{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fc1a93",
   "metadata": {},
   "source": [
    "# Metrics for User Engagement with Search\n",
    "\n",
    "Number and percentage of queries with a dwell time of more than 10 seconds. We're aiming to answer the question of whether users are finding full text search results relevant/useful.\n",
    "\n",
    "We'll start by aggregating this on a daily basis using data from the SearchSatisfaction schema. We want to be able to filter by platform (desktop, mobile, android, iOS), bot vs. non-bot, and project, language edition, and if possible the language used in the search.\n",
    "\n",
    "Looks like browser language headers are not stored in either the SearchSatisfaction nor the CirrusSearchRequest schema in the Data Lake. In other words, we'll either need to start storing that, or store data about user preferences on language (e.g. interface language) in those schemas.\n",
    "\n",
    "What does this look like?\n",
    "\n",
    "1. Identify all search sessions with first full-text SERP on a given day. We'll look one hour prior to the day starting and ignore those that had SERPs in that window.\n",
    "2. Grab checkin events for those search sessions, up until 1 hour after midnight the day we're looking at.\n",
    "3. Get the maximum checkin time for each session.\n",
    "4. Aggregate sessions by maximum session time. All sessions that didn't have a checkin gets a checkin time of 0.\n",
    "5. Store that data in a table.\n",
    "\n",
    "We can then create a specific metric (number of sessions with dwell time of *x*) and create survival curves at will.\n",
    "\n",
    "## Notes ##\n",
    "\n",
    "The partition statement works reasonably well for daily aggregations. If we're doing hourly aggregations, we might want to restructure it so that the amount of data it sifts through is much smaller.\n",
    "\n",
    "Should we calculate the funnel? In other words, number of sessions w/full text search, number of sessions with a click, maximum checkin time for that session. That would allow us to understand not only session length, but also whether users click on events. It'll require two separate tables, because we don't want to duplicate the number of sessions and click counts for each checkin length. We might just want to do that too.\n",
    "\n",
    "Timestamps: we'll coalesce `dt` and `meta.dt`, and trust whatever comes out of it. When I investigated timestamps for searches on Commons I found that there are peaks around the various hour intervals, but they're incredibly small compared to the correct timestamp. To begin with, it's easier to trust these timestamps than develop heuristics to change them.\n",
    "\n",
    "Event logging in SearchSatisfaction is only done on the desktop platform. This means that for these metrics, `platform` will always be `desktop`. We'll still have that column in the dataset to standarize on `project_family`, `wiki_db`, and `platform` as the key categories to filter on across both SearchSatisfaction and CirrusSearch.\n",
    "\n",
    "We want to be able to separate activity by whether the searches are done by a bot, a user, or someone who's likely a bot. We'll adopt the `agent_type` field name used in Wikistats and reuse most of their categories. If the user agent defines the agent as a bot, we'll label it `bot`. If more than 50 searches are done in a session, it's likely a non-human agent and we'll label that `automated`. Otherwise, we'll label it `user`.\n",
    "\n",
    "We define the sessions counts as `INT`, which allows for up to 2 billion search sessions per wiki per day. That should give us room to grow a bit. When we get to it, we can upgrade to `BIGINT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5ca94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from wmfdata import spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06811700",
   "metadata": {},
   "source": [
    "## Configuring Timestamps\n",
    "\n",
    "We'll call the day we're gathering data for `data_day`. We're also expecting this notebook to be run the day after, which we'll call `next_day`. In order to ignore search sessions that started on the previous day, we also define that day. Lastly, we set a limit of one hour after midnight UTC as the cutoff for data. In other words, we expect search sessions to be completed within one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_day = dt.datetime.now(dt.timezone.utc).date()\n",
    "\n",
    "data_day = next_day - dt.timedelta(days = 1)\n",
    "previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b50b98",
   "metadata": {},
   "source": [
    "## Aggregation Tables\n",
    "\n",
    "We define a set of tables in the Data Lake for aggregation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10fd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_session_funnel_table = 'search_dashboard_data.fulltext_funnel_counts'\n",
    "\n",
    "search_checkin_table = 'search_dashboard_data.fulltext_checkin_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67616da",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_funnel_table_query = '''\n",
    "CREATE TABLE {table_name} (\n",
    "    project_family STRING COMMENT \"project family (e.g. 'Wikipedia')\",\n",
    "    wiki_db STRING COMMENT \"database name of the wiki (e.g. 'enwiki')\",\n",
    "    language_code STRING COMMENT \"the language code of the wiki (e.g. 'en' for English, 'ar' for Arabic)\",\n",
    "    platform STRING COMMENT \"desktop, mobile, Android, or iOS\",\n",
    "    agent_type STRING COMMENT \"user, automated, or bot\",\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    num_fulltext_sessions INT COMMENT \"number of search sessions with full text searches\",\n",
    "    num_click_sessions INT COMMENT \"number of sessions with a click on a page in the results\",\n",
    "    num_checkin_sessions INT COMMENT \"number of sessions with at least one checkin event\",\n",
    "    num_click_and_checkin_sessions INT COMMENT \"number of sessions with both click and checkin events\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ca3ad94",
   "metadata": {},
   "source": [
    "print(create_funnel_table_query.format(table_name = search_session_funnel_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee77bb8",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* I don't think `session_length` is a good name for a column, because we're not measuring how long a search session is, we're measuring how long the user spent on any given page they clicked on. I've decided to use `max_checkin` as the naming instead.\n",
    "* I'll use `num_sessions` instead of `session_count` so that the naming is consistent with the funnel table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0497333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_checkin_table_query = '''\n",
    "CREATE TABLE {table_name} (\n",
    "    project_family STRING COMMENT \"project family (e.g. 'Wikipedia')\",\n",
    "    wiki_db STRING COMMENT \"database name of the wiki (e.g. 'enwiki')\",\n",
    "    language_code STRING COMMENT \"the language code of the wiki (e.g. 'en' for English, 'ar' for Arabic)\",\n",
    "    platform STRING COMMENT \"desktop, mobile, Android, or iOS\",\n",
    "    agent_type STRING COMMENT \"user, automated, or bot\",\n",
    "    log_date DATE COMMENT \"the date of the aggregated search counts\",\n",
    "    max_checkin INT COMMENT \"maximum time in seconds for checkin events in a given session\",\n",
    "    num_sessions INT COMMENT \"number of sessions with this maximum checkin\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83daa224",
   "metadata": {},
   "source": [
    "print(create_checkin_table_query.format(table_name = search_checkin_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363666",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c743ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition_statement(start_ts, end_ts, prefix = ''):\n",
    "    '''\n",
    "    This takes the two timestamps and creates a statement that selects\n",
    "    partitions based on `year`, `month`, and `day` in order to make our\n",
    "    data gathering not use excessive amounts of data. It assumes that\n",
    "    `start_ts` and `end_ts` are not more than a month apart, which should\n",
    "    be a reasonable expectation for this notebook.\n",
    "    \n",
    "    An optional prefix can be set to enable selecting partitions for\n",
    "    multiple tables with different aliases.\n",
    "    \n",
    "    :param start_ts: start timestamp\n",
    "    :type start_ts: datetime.datetime\n",
    "    \n",
    "    :param end_ts: end timestamp\n",
    "    :type end_ts: datetime.datetime\n",
    "    \n",
    "    :param prefix: prefix to use in front of partition clauses, \".\" is added automatically\n",
    "    :type prefix: str\n",
    "    '''\n",
    "    \n",
    "    if prefix:\n",
    "        prefix = f'{prefix}.' # adds \".\" after the prefix\n",
    "    \n",
    "    # there are three cases:\n",
    "    # 1: month and year are the same, output a \"BETWEEN\" statement with the days\n",
    "    # 2: months differ, but the years are the same.\n",
    "    # 3: years differ too.\n",
    "    # Case #2 and #3 can be combined, because it doesn't really matter\n",
    "    # if the years are the same in the month-selection or not.\n",
    "    \n",
    "    if start_ts.year == end_ts.year and start_ts.month == end_ts.month:\n",
    "        return(f'''{prefix}year = {start_ts.year}\n",
    "AND {prefix}month = {start_ts.month}\n",
    "AND {prefix}day BETWEEN {start_ts.day} AND {end_ts.day}''')\n",
    "    else:\n",
    "        return(f'''\n",
    "(\n",
    "    ({prefix}year = {start_ts.year}\n",
    "     AND {prefix}month = {start_ts.month}\n",
    "     AND {prefix}day >= {start_ts.day})\n",
    " OR ({prefix}year = {end_ts.year}\n",
    "     AND {prefix}month = {end_ts.month}\n",
    "     AND {prefix}day <= {end_ts.day})\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47921c40",
   "metadata": {},
   "source": [
    "## Number of searches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f073237",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We count the overall number of searches in each session to later label\n",
    "## those with >= 50 searches as \"probably non-human\"\n",
    "\n",
    "## Q: How do we identify all platforms?\n",
    "## A: We don't, SearchSatisfaction is only instrumented on desktop.\n",
    "\n",
    "session_count_query = '''\n",
    "WITH fulltext_sessions AS ( -- all search sessions started during the day of interest with >= 1 fulltext search\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        FIRST_VALUE(wiki) AS wiki_db,\n",
    "        FIRST_VALUE(useragent.is_bot) AS is_bot,\n",
    "        COUNT(1) AS num_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\", 1, NULL)) AS num_fulltext_searches,\n",
    "        MIN(coalesce(client_dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.searchsatisfaction AS ess\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.subTest IS NULL\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    AND event.isforced IS NULL -- only include non-test users\n",
    "    GROUP BY event.searchsessionid\n",
    "    HAVING\n",
    "        TO_DATE(session_start_dt) = \"{today}\"\n",
    "        AND num_fulltext_searches > 0\n",
    "),\n",
    "click_sessions ( -- sessions that had a click on a page or a visit page event\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        1 AS clicked,\n",
    "        MIN(coalesce(client_dt, meta.dt)) AS first_click_dt\n",
    "    FROM fulltext_sessions AS fs\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON fs.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action IN (\"click\", \"visitPage\")\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND coalesce(client_dt, meta.dt) > fs.session_start_dt\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "checkin_sessions ( -- sessions that had a checkin\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        1 AS checked_in\n",
    "    FROM fulltext_sessions AS fs\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON fs.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action = \"checkin\"\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND coalesce(client_dt, meta.dt) > fs.session_start_dt\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "click_and_checkin_sessions ( -- sessions that had a click then a checkin\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        1 AS clicked_and_checked_in\n",
    "    FROM click_sessions AS cs\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON cs.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action = \"checkin\"\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND coalesce(client_dt, meta.dt) > cs.first_click_dt\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "aggregated_sessions AS (\n",
    "    SELECT\n",
    "        fs.wiki_db,\n",
    "        \"desktop\" AS platform,\n",
    "        CASE\n",
    "            WHEN fs.is_bot = true\n",
    "            THEN \"bot\"\n",
    "            WHEN fs.num_searches >= 50\n",
    "            THEN \"automated\"\n",
    "            ELSE \"user\"\n",
    "        END AS agent_type,\n",
    "        TO_DATE(fs.session_start_dt) AS log_date,\n",
    "        COUNT(1) AS num_fulltext_sessions,\n",
    "        COUNT(cs.clicked) AS num_click_sessions,\n",
    "        COUNT(chs.checked_in) AS num_checkin_sessions,\n",
    "        COUNT(cacs.clicked_and_checked_in) AS num_click_and_checkin_sessions\n",
    "    FROM fulltext_sessions AS fs\n",
    "    LEFT JOIN click_sessions AS cs\n",
    "    ON fs.session_id = cs.session_id\n",
    "    LEFT JOIN checkin_sessions AS chs\n",
    "    ON fs.session_id = chs.session_id\n",
    "    LEFT JOIN click_and_checkin_sessions AS cacs\n",
    "    ON fs.session_id = cacs.session_id\n",
    "    GROUP BY wiki_db,\n",
    "        \"desktop\",\n",
    "        CASE\n",
    "            WHEN fs.is_bot = true\n",
    "            THEN \"bot\"\n",
    "            WHEN fs.num_searches >= 50\n",
    "            THEN \"automated\"\n",
    "            ELSE \"user\"\n",
    "        END,\n",
    "        TO_DATE(fs.session_start_dt)\n",
    "),\n",
    "wikis AS (\n",
    "    SELECT\n",
    "        database_code AS wiki_db,\n",
    "        database_group AS project_family,\n",
    "        language_code\n",
    "    FROM canonical_data.wikis\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    wikis.project_family,\n",
    "    aggs.wiki_db,\n",
    "    wikis.language_code,\n",
    "    aggs.platform,\n",
    "    aggs.agent_type,\n",
    "    aggs.log_date,\n",
    "    aggs.num_fulltext_sessions,\n",
    "    aggs.num_click_sessions,\n",
    "    aggs.num_checkin_sessions,\n",
    "    aggs.num_click_and_checkin_sessions\n",
    "FROM aggregated_sessions AS aggs\n",
    "JOIN wikis\n",
    "ON aggs.wiki_db = wikis.wiki_db\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef247393",
   "metadata": {},
   "source": [
    "print(session_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = search_session_funnel_table\n",
    "    ))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63da9406",
   "metadata": {},
   "source": [
    "debug_data = spark.run(session_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        # aggregate_table = search_count_table_name\n",
    "    ))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22ca87ce",
   "metadata": {},
   "source": [
    "debug_data.loc[debug_data['language_code'] == 'fr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30070377",
   "metadata": {},
   "source": [
    "## Dwell Time Aggregation\n",
    "\n",
    "We reuse much of the previous query to get search sessions. In this case, we make the query identify the maximum checkin time in the session, and then count sessions by said maximum. Sessions that did not have a checkin gets their time set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e4723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use the same approach as before to count the number of searches in a session\n",
    "## and label those with more than 50 searches as \"automated\"\n",
    "\n",
    "checkin_length_query = '''\n",
    "WITH fulltext_sessions AS ( -- all search sessions started during the day of interest with >= 1 fulltext search\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        FIRST_VALUE(wiki) AS wiki_db,\n",
    "        FIRST_VALUE(useragent.is_bot) AS is_bot,\n",
    "        COUNT(1) AS num_searches,\n",
    "        COUNT(IF(event.source = \"fulltext\", 1, NULL)) AS num_fulltext_searches,\n",
    "        MIN(coalesce(client_dt, meta.dt)) AS session_start_dt\n",
    "    FROM event.searchsatisfaction AS ess\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.subTest IS NULL\n",
    "    AND event.action = \"searchResultPage\"\n",
    "    AND event.isforced IS NULL -- only include non-test users\n",
    "    GROUP BY event.searchsessionid\n",
    "    HAVING\n",
    "        TO_DATE(session_start_dt) = \"{today}\"\n",
    "        AND num_fulltext_searches > 0\n",
    "),\n",
    "checkin_sessions ( -- sessions that had a checkin\n",
    "    SELECT\n",
    "        event.searchsessionid AS session_id,\n",
    "        MAX(event.checkin) AS max_checkin\n",
    "    FROM fulltext_sessions AS fs\n",
    "    INNER JOIN event.searchsatisfaction AS ess\n",
    "    ON fs.session_id = ess.event.searchsessionid\n",
    "    WHERE {ess_partition_statement}\n",
    "    AND event.action = \"checkin\"\n",
    "    AND coalesce(client_dt, meta.dt) < \"{limit_timestamp}\" -- until end of our data window\n",
    "    AND coalesce(client_dt, meta.dt) > fs.session_start_dt\n",
    "    GROUP BY event.searchsessionid\n",
    "),\n",
    "aggregated_sessions AS (\n",
    "    SELECT\n",
    "        fs.wiki_db,\n",
    "        \"desktop\" AS platform,\n",
    "        CASE\n",
    "            WHEN fs.is_bot = true\n",
    "            THEN \"bot\"\n",
    "            WHEN fs.num_searches >= 50\n",
    "            THEN \"automated\"\n",
    "            ELSE \"user\"\n",
    "        END AS agent_type,\n",
    "        TO_DATE(fs.session_start_dt) AS log_date,\n",
    "        COALESCE(cs.max_checkin, 0) AS max_checkin,\n",
    "        COUNT(1) AS num_sessions\n",
    "    FROM fulltext_sessions AS fs\n",
    "    LEFT JOIN checkin_sessions AS cs\n",
    "    ON fs.session_id = cs.session_id\n",
    "    GROUP BY wiki_db,\n",
    "        \"desktop\",\n",
    "        CASE\n",
    "            WHEN fs.is_bot = true\n",
    "            THEN \"bot\"\n",
    "            WHEN fs.num_searches >= 50\n",
    "            THEN \"automated\"\n",
    "            ELSE \"user\"\n",
    "        END,\n",
    "        TO_DATE(fs.session_start_dt),\n",
    "        COALESCE(cs.max_checkin, 0)\n",
    "),\n",
    "wikis AS (\n",
    "    SELECT\n",
    "        database_code AS wiki_db,\n",
    "        database_group AS project_family,\n",
    "        language_code\n",
    "    FROM canonical_data.wikis\n",
    ")\n",
    "INSERT INTO {aggregate_table}\n",
    "SELECT\n",
    "    wikis.project_family,\n",
    "    aggs.wiki_db,\n",
    "    wikis.language_code,\n",
    "    aggs.platform,\n",
    "    aggs.agent_type,\n",
    "    aggs.log_date,\n",
    "    aggs.max_checkin,\n",
    "    aggs.num_sessions\n",
    "FROM aggregated_sessions AS aggs\n",
    "JOIN wikis\n",
    "ON aggs.wiki_db = wikis.wiki_db\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd6e645f",
   "metadata": {},
   "source": [
    "debug_data = spark.run(checkin_length_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        # aggregate_table = search_checkin_table\n",
    "    ))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9474937b",
   "metadata": {},
   "source": [
    "debug_data.loc[(debug_data['wiki_db'] == 'nowiki'), 'num_sessions'].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e765827",
   "metadata": {},
   "source": [
    "debug_data.loc[(debug_data['wiki_db'] == 'nowiki') &\n",
    "               (debug_data['max_checkin'] > 0), 'num_sessions'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304cf89",
   "metadata": {},
   "source": [
    "I've confirmed that the total number of sessions counted for 2021-07-12 for `nowiki` matches the number of search sessions counted in the overall session counts. I've also confirmed that the number of sessions with a checkin time `> 0` matches the `num_checkin_sessions` in the overall session count. This looks good to go to me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404b77b",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "Currently we've gathered about a month and a half of data to build a dashboard on for the team to play around with and gather feedback. If it's good to go, we can set up daily updates."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3475ce21",
   "metadata": {},
   "source": [
    "try:\n",
    "    spark.run(search_count_query.format(\n",
    "        today = data_day,\n",
    "        limit_timestamp = limit_timestamp.isoformat(),\n",
    "        ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "        ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "        aggregate_table = search_count_table_name\n",
    "    ))\n",
    "except UnboundLocalError:\n",
    "    # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "    # and so we ignore that error\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04aaa182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up days\n",
    "first_day = dt.date(2021, 6, 2) # grab data from the first day of June 2021\n",
    "last_day = dt.date(2021, 7, 13) # running on current day, so we have data until July 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f87f465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-01 (simulating cron job on 2021-06-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-02 (simulating cron job on 2021-06-03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-03 (simulating cron job on 2021-06-04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-04 (simulating cron job on 2021-06-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-05 (simulating cron job on 2021-06-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-06 (simulating cron job on 2021-06-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-07 (simulating cron job on 2021-06-08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-08 (simulating cron job on 2021-06-09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-09 (simulating cron job on 2021-06-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-10 (simulating cron job on 2021-06-11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-11 (simulating cron job on 2021-06-12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-12 (simulating cron job on 2021-06-13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-13 (simulating cron job on 2021-06-14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-14 (simulating cron job on 2021-06-15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-15 (simulating cron job on 2021-06-16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-16 (simulating cron job on 2021-06-17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-17 (simulating cron job on 2021-06-18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-18 (simulating cron job on 2021-06-19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-19 (simulating cron job on 2021-06-20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-20 (simulating cron job on 2021-06-21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-21 (simulating cron job on 2021-06-22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-22 (simulating cron job on 2021-06-23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-23 (simulating cron job on 2021-06-24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-24 (simulating cron job on 2021-06-25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-25 (simulating cron job on 2021-06-26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-26 (simulating cron job on 2021-06-27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-27 (simulating cron job on 2021-06-28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-28 (simulating cron job on 2021-06-29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-29 (simulating cron job on 2021-06-30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-06-30 (simulating cron job on 2021-07-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-01 (simulating cron job on 2021-07-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-02 (simulating cron job on 2021-07-03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-03 (simulating cron job on 2021-07-04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-04 (simulating cron job on 2021-07-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-05 (simulating cron job on 2021-07-06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-06 (simulating cron job on 2021-07-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-07 (simulating cron job on 2021-07-08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-08 (simulating cron job on 2021-07-09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-09 (simulating cron job on 2021-07-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-10 (simulating cron job on 2021-07-11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-11 (simulating cron job on 2021-07-12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running data gathering for 2021-07-12 (simulating cron job on 2021-07-13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "current_day = first_day\n",
    "\n",
    "while current_day <= last_day:\n",
    "    # calculate days\n",
    "    next_day = current_day\n",
    "    data_day = next_day - dt.timedelta(days = 1)\n",
    "    previous_day = data_day - dt.timedelta(days = 1)\n",
    "\n",
    "    limit_timestamp = dt.datetime.combine(next_day, dt.time(hour = 1))\n",
    "    \n",
    "    # print some helpful stuff\n",
    "    print(f'running data gathering for {data_day} (simulating cron job on {current_day})')\n",
    "    \n",
    "    try:\n",
    "        spark.run(checkin_length_query.format(\n",
    "            today = data_day,\n",
    "            limit_timestamp = limit_timestamp.isoformat(),\n",
    "            ess_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ess'),\n",
    "            ms_partition_statement = make_partition_statement(previous_day, next_day, prefix = 'ms'),\n",
    "            aggregate_table = search_checkin_table\n",
    "        ))\n",
    "    except UnboundLocalError:\n",
    "        # wmfdata currently (late Feb 2021) has an issue with DDL/DML SQL queries,\n",
    "        # and so we ignore that error\n",
    "        pass\n",
    "    \n",
    "    current_day += dt.timedelta(days = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
